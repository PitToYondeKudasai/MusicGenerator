{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Variational Encoder "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we implement a encoder similar to the one explained in the paper\n",
    "https://arxiv.org/abs/1803.05428"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Libraries useful for the test dataset\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_sequences(input_data, tw):\n",
    "    input_seq = []\n",
    "    L = len(input_data)\n",
    "    for i in range(L-tw):\n",
    "        train_seq = input_data[i:i+tw]\n",
    "        input_seq.append(train_seq)\n",
    "    return input_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we create a set sequences of data\n",
    "flight_data = sns.load_dataset(\"flights\")\n",
    "all_data = flight_data['passengers'].values.astype(float)\n",
    "test_data_size = 12\n",
    "\n",
    "train_data = all_data[:-test_data_size]\n",
    "test_data = all_data[-test_data_size:]\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "train_data_normalized = scaler.fit_transform(train_data .reshape(-1, 1))\n",
    "train_data_normalized = torch.FloatTensor(train_data_normalized).view(-1)\n",
    "train_window = 12\n",
    "train_input_seq = create_input_sequences(train_data_normalized, train_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input_size is the size of the sequence\n",
    "# The latent_dim is the dimension of the latent space\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, latent_dim):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.latent_dim = latent_dim        \n",
    "        self.hidden_cell = (torch.zeros(2,1,self.input_size),\n",
    "                            torch.zeros(2,1,self.input_size))\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size, input_size, bidirectional = True)\n",
    "        self.mu = nn.Linear(2*input_size, latent_dim)\n",
    "        self.logvar = nn.Linear(2*input_size, latent_dim)\n",
    "        \n",
    "    def sample(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "          \n",
    "    def reset_hidden(self):\n",
    "        self.hidden_cell = (torch.zeros(2,1,self.input_size),\n",
    "                            torch.zeros(2,1,self.input_size))\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        lstm_out, self.hidden_cell = self.lstm(input_seq.view(-1 ,1, len(input_seq)), self.hidden_cell)\n",
    "        hidden_seq = torch.cat((self.hidden_cell[0][0], self.hidden_cell[0][1]), 1)\n",
    "        mu = self.mu(hidden_seq)\n",
    "        logvar = self.logvar(hidden_seq)\n",
    "        return self.sample(mu, logvar), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(12,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_space_mapping = encoder(train_input_seq[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.9346,  0.3562, -0.1516,  0.2647]], grad_fn=<AddBackward0>),\n",
       " tensor([[-0.0870,  0.0698,  0.1671, -0.0056]], grad_fn=<AddmmBackward>),\n",
       " tensor([[ 0.0080, -0.0131, -0.0109,  0.2130]], grad_fn=<AddmmBackward>))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_space_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
